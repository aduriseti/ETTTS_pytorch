{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "#### High level\n",
    " - [ ] get any NLP network working\n",
    " - [ ] get any audio network working\n",
    " - [ ] try\n",
    "     - [ ] chainer - https://docs.chainer.org/en/stable/\n",
    "     - [ ] gluon - https://medium.com/apache-mxnet/mxnet-gluon-in-60-minutes-3d49eccaf266\n",
    "     - [x] pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETTTS - convolutional TTS \n",
    "- [ ] https://arxiv.org/abs/1710.08969\n",
    "    - [x] read\n",
    "    - [x] understand math\n",
    "    - [x] draw architecture\n",
    "    - [ ] implement in pytorch\n",
    "        - [x] get data\n",
    "        - [x] preprocess data\n",
    "        - [x] char embed\n",
    "        - [x] 1d conv\n",
    "            - [x] fix causality\n",
    "        - [x] 1d transpose conv oooooh\n",
    "        - [x] highway connection/highway convolution\n",
    "        - [x] weights initialize\n",
    "        - [x] textenc\n",
    "        - [x] audioenc\n",
    "        - [x] attention\n",
    "            - [x] guided\n",
    "            - [ ] forcibly incremental\n",
    "        - [x] audiodec\n",
    "        - [x] ssrn\n",
    "        - [x] impl loss functions\n",
    "        - [x] train text2Mel\n",
    "            - [ ] why does it sometimes take 1/5 the time for regular 1d convs to train?\n",
    "        - [x] train SSRN\n",
    "        - [x] get GPU training working\n",
    "            - [x] collab\n",
    "            - [x] google cloud\n",
    "            - [x] make backwards compatible w/ CPU\n",
    "        - [x] bigger batch size - gpu mem usge at < 10%\n",
    "            - [x] might have to increase cores for dataloader - 5 cores about saturates gpu  at batch size 16\n",
    "            - [x] pretty sure model limited by fetcher speed\n",
    "        - [x] checkpoint models % training\n",
    "            - [x] remember to call model.eval() on load chkpt to make sure layers are in evaluation (as opposed to training mode) \n",
    "            - [ ] different checkpoint paths for different model params\n",
    "            - [x] combine checkpointing logic for text2mel and ssrn by combining the text2Mel,audioDec,attention models into one class\n",
    "        - [ ] test out if concatenating mel and text enc makes sense\n",
    "        - [x] combine the text2Mel,audioDec,attention models into one class\n",
    "        - [x] generate text2Mel\n",
    "        - [x] generate SSRN\n",
    "        - [x] fix inference memory leak\n",
    "            - with ch.no_grad()\n",
    "        - [ ] create class for general pytorch training/checkpointing/loss monitoring\n",
    "        - [ ] chunked generation - train network to encode multiple timesteps at a time\n",
    "        - [x] hyperparams class\n",
    "            - [ ] add initialization?\n",
    "        - [x] separate training code from model code\n",
    "        - [ ] set behavior at preempt to restart and resume training \n",
    "        - [ ] split train test\n",
    "        - [ ] normalization\n",
    "            - [x] batch norm\n",
    "            - [ ] layer norm\n",
    "            - [ ] instance norm\n",
    "            - [ ] group norm\n",
    "        - [ ] decay\n",
    "        - [ ] gradient clipping\n",
    "        - [ ] pad from other direction? - seemed like attention model trained from end of input to beginning\n",
    "        - [ ] get rid of unnecesary separability params for separable convolutions\n",
    "        - [x] get some NULL character going for padding - alternatively modify c2i to not map any character to 0\n",
    "- [x] use as reference\n",
    "    - [x] https://github.com/Kyubyong/dc_tts\n",
    "    - [x] https://github.com/eazhary/dctts2\n",
    "    - [x] https://github.com/joisino/chainer-ETTTS\n",
    "    - [x] find difference\n",
    "- [ ] citations\n",
    "        - [ ] main insipration: https://arxiv.org/abs/1705.03122\n",
    "- [ ] cited by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- [ ] waveRNN\n",
    "    - [ ] https://arxiv.org/pdf/1802.08435.pdf\n",
    "    - [ ] /Users/aduriseti/Documents/2018spring/tesla/WaveRNN-master\n",
    "    - [ ] /Users/aduriseti/Documents/2018spring/tesla/TensorFlow-Efficient-Neural-Audio-Synthesis-master\n",
    "- [ ] waveNet\n",
    "    - [ ] https://arxiv.org/abs/1609.03499\n",
    "- [ ] streaming spectrogram generation\n",
    "    - [ ] https://pdfs.semanticscholar.org/095a/ce7fbffb4b55ba6e71f6c06566fa4de67d69.pdf\n",
    "- [ ] gan TTS/voice conversion (VC)\n",
    "    - [ ] https://github.com/r9y9/gantts\n",
    "- [ ] styleNN - if only for the dataset\n",
    "    - [ ] http://imanmalik.com/cs/2017/06/05/neural-style.html\n",
    "- [ ] deepVoice3\n",
    "    - [ ] https://arxiv.org/abs/1710.07654\n",
    "- [ ] general optimization\n",
    "    - [ ] squeezenet\n",
    "        - [ ] https://arxiv.org/abs/1602.07360\n",
    "    - [ ] mobilenet\n",
    "        - [x] https://arxiv.org/abs/1704.04861\n",
    "        - depthwise separable /w memory managemnt opt and op vectorizing opt\n",
    "        - [x] understand depthwise sep & complexity\n",
    "        - [x] understand memory management opt\n",
    "        - [x] understand op opt\n",
    "        - [ ] look at related papers\n",
    "        - [x] try out channel thinning parameter $\\alpha$\n",
    "    - [x] xception:\n",
    "        - [x] https://arxiv.org/abs/1610.02357\n",
    "        - pure depthwise separable w/ residual connections - demonstrated state of the art performance on ImageNet and faster training\n",
    "    - [ ] depthwise sep convolution for NMT\n",
    "        - [x] https://openreview.net/forum?id=S1jBcueAb\n",
    "        - super separability: group ptwise ops also\n",
    "        - they found parameter savings from serparation/super separation to be superior to param savings from dilation\n",
    "        - [x] try it out \n",
    "    - [ ] sparsity constraints w/ pruning\n",
    "        - [ ] for RNN i.e. waveRNN\n",
    "        - [ ] forn CNN - saw package online - https://github.com/jacobgil/pytorch-pruning\n",
    "            - Note: supposedly not equally efficient to train\n",
    "    - [ ] network weight decomposition\n",
    "        - [ ] https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
